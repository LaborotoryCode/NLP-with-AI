from nltk.tokenize import word_tokenize

points = 0

text = """Kids were laughing in my classes, while I was scheming for the masses. Who do you think you are? 
Dreaming 'bout being a big star."""

tokenized_word=word_tokenize(text)
#split up each word